# #logger/longtime.py

import csv
import os
import time
import threading
from datetime import datetime, timedelta
from scpi.waveform import get_channel_waveform_data
from scpi.interface import safe_query
from scpi.interface import scpi_lock
from app.app_state import is_logging_active
import app.app_state as app_state
from scpi.data import scpi_data
from utils.debug import log_debug, set_debug_level

is_logging = False
pause_flag = False
stop_flag = False

def get_all_channels_waveform_data(scope, channels, use_simple_calc=True):
    """
    Optimized function to get waveform data for multiple channels.
    If your SCPI interface supports batch operations, implement that here.
    Otherwise, this function minimizes the individual calls.
    """
    results = {}
    for ch in channels:
        try:
            vpp, vavg, vrms = get_channel_waveform_data(scope, ch, use_simple_calc=use_simple_calc)
            results[ch] = (vpp, vavg, vrms)
        except Exception as e:
            log_debug(f"‚ö†Ô∏è Error getting data for channel {ch}: {e}")
            results[ch] = (None, None, None)
    return results

def cache_channel_metadata(scope, channels, current_scale):
    """
    Cache channel units and calculate scaling factors once at the start.
    """
    channel_units = {}
    channel_scales = {}
    channel_names = {}
    
    log_debug("‚öôÔ∏è Caching channel metadata...", level="MINIMAL")
    
    for ch in channels:
        # Cache channel names
        channel_names[ch] = f"CH{ch}" if isinstance(ch, int) else ch
        
        # Cache channel units
        try:
            chnum = str(ch).replace("CH", "").strip()
            with scpi_lock:
                unit = safe_query(scope, f":CHAN{chnum}:UNIT?", default="VOLT").strip().upper()
            channel_units[ch] = unit
            
            # Pre-calculate scaling factors
            if unit == "AMP":
                channel_scales[ch] = 1.0
                log_debug(f"‚öôÔ∏è {channel_names[ch]} is in AMP ‚Äî no scaling applied")
            else:
                channel_scales[ch] = current_scale
                log_debug(f"‚öôÔ∏è {channel_names[ch]} is in VOLT ‚Äî applying scale {current_scale}")
                
        except Exception as e:
            log_debug(f"‚ö†Ô∏è Unit detection failed for {channel_names[ch]}: {e}")
            channel_units[ch] = "VOLT"  # default
            channel_scales[ch] = current_scale
    
    return channel_units, channel_scales, channel_names

def start_logging(_scope_unused, ip, channels, duration, interval, vavg_enabled, vrms_enabled, status_callback, current_scale=1.0):
    if app_state.is_power_analysis_active:
        status_callback("‚ö†Ô∏è Cannot start long-time logging during power analysis.")
        return
    from app.app_state import scope
    if not scope:
        log_debug("‚ùå Scope not connected")
        return
    global is_logging, pause_flag, stop_flag

    if is_logging:
        status_callback("‚ö†Ô∏è Already running")
        return

    session_dir = f"oszi_csv/session_{datetime.now():%Y%m%d_%H%M%S}"
    os.makedirs(session_dir, exist_ok=True)
    csv_path = os.path.join(session_dir, "session_log.csv")
    total = int((duration * 3600) // interval)
    end_time = datetime.now() + timedelta(seconds=duration * 3600)

    log_debug(f"üìÅ Logging to {csv_path}", level="MINIMAL")
    log_debug(f"üïí Estimated end time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", level="MINIMAL")

    is_logging = True
    pause_flag = False
    stop_flag = False
    app_state.is_logging_active = True

    def loop():
        # Get scope ID once at the start
        with scpi_lock:
            scope_id = safe_query(scope, '*IDN?', 'N/A')
        log_debug(f"üß™ Logging scope ID: {scope_id}", level="MINIMAL")
        
        nonlocal csv_path
        try:
            # Cache all channel metadata once at the start
            channel_units, channel_scales, channel_names = cache_channel_metadata(scope, channels, current_scale)
            
            with open(csv_path, "w", newline="") as f:
                writer = csv.writer(f)
                
                # Build header using cached names
                header = ["Timestamp"]
                for ch in channels:
                    chname = channel_names[ch]
                    header.append(f"{chname}_Vpp")
                    if vavg_enabled:
                        header.append(f"{chname}_Vavg")
                    if vrms_enabled:
                        header.append(f"{chname}_Vrms")
                writer.writerow(header)

                start_time = time.time()
                status_update_counter = 0

                for i in range(total):
                    if stop_flag:
                        log_debug("üõë Logging stopped by user", level="MINIMAL")
                        break

                    # Handle pause state
                    if pause_flag:
                        status_callback("‚è∏ Paused")
                    while pause_flag and not stop_flag:
                        time.sleep(0.5)
                    if stop_flag:
                        log_debug("üõë Logging stopped during pause", level="MINIMAL")
                        break

                    # Get timestamp once
                    timestamp = datetime.now().isoformat()
                    row = [timestamp]
                    
                    # Single SCPI operation to get all channel data
                    with scpi_lock:
                        all_channel_data = get_all_channels_waveform_data(scope, channels, use_simple_calc=True)
                    
                    # Process all channels outside the lock
                    for ch in channels:
                        vpp, vavg, vrms = all_channel_data[ch]
                        chname = channel_names[ch]
                        scale = channel_scales[ch]
                        
                        # Debug output (reduce frequency for performance)
                        if i % 10 == 0:  # Only log every 10th sample
                            log_debug(f"{chname} ‚ûú Vpp={vpp:.3f}  Vavg={vavg:.3f}  Vrms={vrms:.3f}")

                        # Add scaled values to row
                        row.append(f"{vpp * scale:.4f}" if vpp is not None else "")
                        if vavg_enabled:
                            row.append(f"{vavg * scale:.4f}" if vavg is not None else "")
                        if vrms_enabled:
                            row.append(f"{vrms * scale:.4f}" if vrms is not None else "")

                    writer.writerow(row)
                    
                    # Reduce debug output frequency for better performance
                    if i % 10 == 0:  # Only log every 10th sample
                        log_debug(f"üìà Sample {i+1}/{total} complete")
                    
                    # Update status less frequently to reduce overhead
                    status_update_counter += 1
                    if status_update_counter % 5 == 0 or i == total - 1:
                        status_callback(f"‚úÖ Saved {i+1}/{total}")

                    # Timing control - calculate next sample time
                    actual_sample_time = time.time()
                    next_sample = actual_sample_time + interval
                    delay = next_sample - time.time()

                    if delay > 0:
                        time.sleep(delay)
                    else:
                        # Only log timing warnings occasionally to reduce overhead
                        if i % 20 == 0:
                            log_debug(f"‚ö†Ô∏è Behind schedule by {-delay:.2f}s")

                # Flush file to ensure data is written
                f.flush()
                os.fsync(f.fileno())

            log_debug("‚úÖ Logging finished", level="MINIMAL")
            status_callback("‚úÖ Logging finished")
            log_debug("‚úÖ Long-time logging completed successfully", level="MINIMAL")

        except Exception as e:
            log_debug(f"‚ùå Logging error: {e}", level="MINIMAL")
            status_callback("‚ùå Error ‚Äî see Debug Log")

        finally:
            global is_logging
            is_logging = False
            app_state.is_logging_active = False

    threading.Thread(target=loop, daemon=True).start()

def pause_resume():
    global pause_flag
    pause_flag = not pause_flag
    return pause_flag

def stop_logging():
    global stop_flag
    stop_flag = True